import argparse
import os
import json
from dotenv import load_dotenv
from agent_init import get_gemini_agent

def produce_dataset(limit=5):
    """
    Orchestrates the generation of creative prompts and pushes them to HF Hub.
    """
    load_dotenv()
    
    # 1. Initialize the Gemini 2.0 Agent (Avoids HF Provider costs, high intelligence)
    print(f"üöÄ Initializing Gemini 2.0 Agent for {limit} generation(s)...", flush=True)
    agent = get_gemini_agent()
    
    # 2. Define the concepts (In a real scenario, this could be loaded from a file or generated by another agent)
    # For this demo/production script, we'll start with a small seed list
    seed_concepts = [
        "A cyberpunk cafe in neon rain",
        "A hyper-realistic close-up of a futuristic robot eye",
        "A cinematic long shot of an ancient library floating in space",
        "An underwater city with bioluminescent buildings",
        "A steampunk laboratory with brass machinery and steam"
    ]
    
    # If the limit is higher than the seed list, we can have the agent expand it
    if limit > len(seed_concepts):
        print(f"üìä Expand seed concepts to {limit}...", flush=True)
        expansion_prompt = f"Given these concepts: {seed_concepts}, generate {limit - len(seed_concepts)} more unique and diverse creative video concepts. Return them as a simple comma-separated list."
        new_concepts_str = agent.run(expansion_prompt)
        # Simple parsing (agent might return extra text, but we'll try to split)
        new_concepts = [c.strip() for c in new_concepts_str.split(",") if c.strip()]
        seed_concepts.extend(new_concepts[:limit - len(seed_concepts)])

    # 3. Process the concepts
    print(f"üé¨ Starting batch generation of {limit} prompts...", flush=True)
    for i, concept in enumerate(seed_concepts[:limit]):
        print(f"\n[{i+1}/{limit}] Processing: {concept}", flush=True)
        try:
            # The agent uses 'generate_synthetic_prompt' to create the high-fidelity prompt
            # and 'push_to_hf_hub' to persist it to the Hub
            orchestration_prompt = f"""
            1. Use 'generate_synthetic_prompt' to convert '{concept}' into a high-fidelity LTX-Video prompt.
            2. The output will be a dictionary-like string.
            3. Convert that output into a Python dictionary.
            4. Use 'push_to_hf_hub' to push that dictionary to 'Limbicnation/Video-Diffusion-Prompt-Style'.
            5. Return the result of the push.
            """
            result = agent.run(orchestration_prompt)
            print(f"‚úÖ Result: {result}", flush=True)
        except Exception as e:
            print(f"‚ùå Error processing '{concept}': {e}", flush=True)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Produce synthetic video prompts and push to HF Hub.")
    parser.add_argument("--limit", type=int, default=5, help="Number of prompts to generate.")
    args = parser.parse_args()
    
    produce_dataset(limit=args.limit)
